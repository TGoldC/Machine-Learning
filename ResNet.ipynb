{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "In this notebook you will gain some hands-on experience with [PyTorch](https://pytorch.org/), one of the major frameworks for deep learning. To install PyTorch. follow [the official installation instructions](https://pytorch.org/get-started/locally/). Make sure that you select the correct OS & select the version with CUDA if your computer supports it.\n",
    "If you do not have an Nvidia GPU, you can install the CPU version by setting `CUDA` to `None`.\n",
    "However, in this case we recommend using [Google Colab](https://colab.research.google.com/).\n",
    "Make sure that you enable GPU acceleration in `Runtime > Change runtime type`.\n",
    "\n",
    "You will start by re-implementing some common features of deep neural networks (dropout and batch normalization) and then implement a very popular modern architecture for image classification (ResNet) and improve its training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dropout\n",
    "Dropout is a form of regularization for neural networks. It works by randomly setting activations (values) to 0, each one with equal probability `p`. The values are then scaled by a factor $\\frac{1}{1-p}$ to conserve their mean.\n",
    "\n",
    "Dropout effectively trains a pseudo-ensemble of models with stochastic gradient descent. During evaluation we want to use the full ensemble and therefore have to turn off dropout. Use `self.training` to check if the model is in training or evaluation mode.\n",
    "\n",
    "Do not use any dropout implementation from PyTorch for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Dropout, as discussed in the lecture and described here:\n",
    "    https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "    \n",
    "    Args:\n",
    "        p: float, dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The module's forward pass.\n",
    "        This has to be implemented for every PyTorch module.\n",
    "        PyTorch then automatically generates the backward pass\n",
    "        by dynamically generating the computational graph during\n",
    "        execution.\n",
    "        \n",
    "        Args:\n",
    "            input: PyTorch tensor, arbitrary shape\n",
    "\n",
    "        Returns:\n",
    "            PyTorch tensor, same shape as input\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Set values randomly to 0.\n",
    "        input_array = input.numpy()\n",
    "        input_flatten = input_array.flatten()\n",
    "        random_num = np.random.choice(a = input_flatten.shape[0] ,size = int(input_flatten.shape[0] * self.p) ,replace = False)\n",
    "        for num in random_num:\n",
    "            input_flatten[num] = 0\n",
    "        input_flatten /= (1-self.p)\n",
    "        input_flatten.reshape(input_array.shape)\n",
    "        \n",
    "        return  torch.from_numpy(input_flatten)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dropout\n",
    "test = torch.ones(10000)\n",
    "dropout = Dropout(0.5)\n",
    "test_dropped = dropout(test)\n",
    "#print(test_dropped)\n",
    "\n",
    "# These assertions can in principle fail due to bad luck, but\n",
    "# if implemented correctly they should almost always succeed.\n",
    "assert np.isclose(test_dropped.sum().item(), 10_000, atol=400)\n",
    "assert np.isclose((test_dropped > 0).sum().item(), 5_000, atol=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch normalization\n",
    "Batch normalization is a trick use to smoothen the loss landscape and improve training. It is defined as the function\n",
    "$$y = \\frac{x - \\mu_x}{\\sigma_x + \\epsilon} \\cdot \\gamma + \\beta$$,\n",
    "where $\\gamma$ and $\\beta$ and learnable parameters and $\\epsilon$ is a some small number to avoid dividing by zero. The Statistics $\\mu_x$ and $\\sigma_x$ are taken separately for each feature. In a CNN this means averaging over the batch and all pixels.\n",
    "\n",
    "Do not use any batch normalization implementation from PyTorch for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Batch normalization, as discussed in the lecture and similar to\n",
    "    https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d\n",
    "    \n",
    "    Only uses batch statistics (no running mean for evaluation).\n",
    "    Batch statistics are calculated for a single dimension.\n",
    "    Gamma is initialized as 1, beta as 0.\n",
    "    \n",
    "    Args:\n",
    "        num_features: Number of features to calculate batch statistics for.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize the required parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Batch normalization over the dimension C of (N, C, L).\n",
    "        \n",
    "        Args:\n",
    "            input: PyTorch tensor, shape [N, C, L]\n",
    "            \n",
    "        Return:\n",
    "            PyTorch tensor, same shape as input\n",
    "        \"\"\"\n",
    "        eps = 1e-5\n",
    "        feature_mean,feature_var =  [],[]\n",
    "        \n",
    "        for i in range(input.shape[1]):\n",
    "            feature_mean.append(input[:,i,:].mean().item())\n",
    "            feature_var.append(input[:,i,:].var().item())\n",
    "        \n",
    "        feature_mean_ts = torch.from_numpy(np.array(feature_mean)).float()\n",
    "        feature_var_ts = torch.from_numpy(np.array(feature_var)).float()\n",
    "       \n",
    "        output = ((input -  feature_mean_ts[None,:,None]) / torch.sqrt(feature_var_ts[None,:,None] + eps)) * self.gamma[None,:,None] + self.beta [None,:,None] \n",
    "        return output\n",
    "        \n",
    "        # TODO: Implement the required transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests the batch normalization implementation\n",
    "torch.random.manual_seed(42)\n",
    "test = torch.randn(8, 2, 4)\n",
    "\n",
    "b1 = BatchNorm(2)\n",
    "test_b1 = b1(test)\n",
    "\n",
    "b2 = nn.BatchNorm1d(2, affine=False, track_running_stats=False)\n",
    "test_b2 = b2(test)\n",
    "\n",
    "\n",
    "assert torch.allclose(test_b1, test_b2, rtol=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ResNet\n",
    "ResNet is the models that first introduced residual connections (a form of skip connections). It is a rather simple, but successful and very popular architecture. In this part of the exercise we will re-implement it step by step.\n",
    "\n",
    "Note that there is also an [improved version of ResNet](https://arxiv.org/abs/1603.05027) with optimized residual blocks. Here we will implement the [original version](https://arxiv.org/abs/1512.03385) for CIFAR-10. Your dropout and batchnorm implementations won't help you here. Just use PyTorch's own layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a convenience function to make e.g. `nn.Sequential` more flexible. It is e.g. useful in combination with `x.squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing the residual blocks. The block is illustrated by this sketch:\n",
    "\n",
    "![Residual connection](img/residual_connection.png)\n",
    "\n",
    "Note that we use 'SAME' padding, no bias, and batch normalization after each convolution. You do not need `nn.Sequential` here. The skip connection is already implemented as `self.skip`. It can handle different strides and increases in the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual block used by ResNet.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first convolution\n",
    "        stride: Stride size of the first convolution, used for downsampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()        \n",
    "        if stride > 1 or in_channels != out_channels:\n",
    "            # Add strides in the skip connection and zeros for the new channels.\n",
    "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
    "                                               (0, 0, 0, 0, 0, out_channels - in_channels),\n",
    "                                               mode=\"constant\", value=0))\n",
    "        else:\n",
    "            self.skip = nn.Sequential()\n",
    "            \n",
    "        # TODO: Initialize the required layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1,bias=False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1,bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)#为什么是out_channels?\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # TODO: Execute the required layers and functions\n",
    "        residual = input\n",
    "        x = input\n",
    "        x = self.relu(self.bn(self.conv1(x)))\n",
    "        x = self.bn(self.conv2(x))\n",
    "        x += self.skip(residual)\n",
    "        output = self.relu(x)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement a stack of residual blocks for convenience. The first layer in the block is the one changing the number of channels and downsampling. You can use `nn.ModuleList` to use a list of child modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual blocks.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first layer\n",
    "        stride: Stride size of the first layer, used for downsampling\n",
    "        num_blocks: Number of residual blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize the required layers (blocks)\n",
    "        if num_blocks == 1 :\n",
    "            self.modulelist = nn.ModuleList([ResidualBlock(in_channels,out_channels,stride)])\n",
    "        else :\n",
    "            self.modulelist = nn.ModuleList([ResidualBlock(in_channels,out_channels,stride)])\n",
    "            for i in range (num_blocks - 1):\n",
    "                self.modulelist.append(ResidualBlock(out_channels,out_channels))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # TODO: Execute the layers (blocks)\n",
    "        for mod in self.modulelist:\n",
    "            input = mod(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to implement the full model! To do this, use the `nn.Sequential` API and carefully read the following paragraph from the paper (Fig. 3 is not important):\n",
    "\n",
    "![ResNet CIFAR10 description](img/resnet_cifar10_description.png)\n",
    "\n",
    "Note that a convolution layer is always convolution + batch norm + activation (ReLU), that each ResidualBlock contains 2 layers, and that you might have to `squeeze` the embedding before the dense (fully-connected) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "num_classes = 10\n",
    "\n",
    "# TODO: Implement ResNet via nn.Sequential\n",
    "resnet = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1,bias=False),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    ResidualStack(in_channels=16,out_channels=16,stride=1,num_blocks=n), \n",
    "    ResidualStack(in_channels=16,out_channels=32,stride=2,num_blocks=n), \n",
    "    ResidualStack(in_channels=32,out_channels=64,stride=2,num_blocks=n),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: torch.squeeze(x)),\n",
    "    nn.Linear(64, num_classes),\n",
    "         \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to initialize the weights of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "resnet.apply(initialize_weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training\n",
    "So now we have a shiny new model, but that doesn't really help when we can't train it. So that's what we do next.\n",
    "\n",
    "First we need to load the data. Note that we split the official training data into train and validation sets, because you must not look at the test set until you are completely done developing your model and report the final results. Some people don't do this properly, but you should not copy other people's bad habits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Get a subset of the CIFAR10 dataset, according to the passed indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, idx=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if idx is None:\n",
    "            return\n",
    "        \n",
    "        self.data = self.data[idx]\n",
    "        targets_np = np.array(self.targets)\n",
    "        self.targets = targets_np[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define transformations that change the images into PyTorch tensors, standardize the values according to the precomputed mean and standard deviation, and provide data augmentation for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ntrain = 45_000\n",
    "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain),\n",
    "                          download=True, transform=transform_train)\n",
    "val_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain,50_000),\n",
    "                        download=True, transform=transform_eval)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=128,\n",
    "                                                   shuffle=True, num_workers=0,#每个epoch执行之前重新划分minibatch\n",
    "                                                   pin_memory=False)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=128,\n",
    "                                                 shuffle=False, num_workers=0,\n",
    "                                                 pin_memory=False)\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(test_set, batch_size=128,\n",
    "                                                  shuffle=False, num_workers=0,\n",
    "                                                  pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we push the model to our GPU (if there is one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "resnet.to(device);\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a helper method that does one epoch of training or evaluation. We have only defined training here, so you need to implement the necessary changes for evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloader, train):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        dataloader: Dataloader providing the data to run our model on\n",
    "        train: Whether this epoch is used for training or evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    # TODO: Change the necessary parts to work correctly during evaluation (train=False)\n",
    "    #print(\"runepoch running\")\n",
    "    device = next(model.parameters()).device\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "   \n",
    "    #print(\"going to enter train mode\")\n",
    "    if train :\n",
    "        model.train() # Set model to training mode (for e.g. batch normalization, dropout)\n",
    "        #print(\"runepoch model.train done\")\n",
    "        i = 0\n",
    "        for xb, yb in dataloader:#一个xb是一个minibatch\n",
    "            #print(xb.shape)\n",
    "            #print(yb.shaoe)\n",
    "            #print(\"going to transfer data to cpu\")\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            #print(\"transfer done\")\n",
    "            #with torch.set_grad_enabled(True):\n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncorrect = torch.sum(top1 == yb)\n",
    "            #print(\"current epoch forward done\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            #print(\"current epoch backward done\")\n",
    "                \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += ncorrect.item()\n",
    "            print(\"loss and accuracy at minibatch {}!\".format(i))\n",
    "            print(epoch_loss)\n",
    "            print(epoch_acc)\n",
    "            i += 1\n",
    "                \n",
    "            \n",
    "    else :\n",
    "        model.eval()\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncorrect = torch.sum(top1 == yb)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += ncorrect.item() \n",
    "     \n",
    "    \n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    epoch_acc /= len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement a method for fitting (training) our model. For many models early stopping can save a lot of training time. Your task is to add early stopping to the loop (based on validation accuracy). Early stopping usually means exiting the training loop if the validation accuracy hasn't improved for `patience` number of steps. Don't forget to save the best model parameters according to validation accuracy. You will need `copy.deepcopy` and the `state_dict` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
    "    \"\"\"\n",
    "    Fit the given model on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        lr_scheduler: Learning rate scheduler that improves training\n",
    "                      in late epochs with learning rate decay\n",
    "        dataloaders: Dataloaders for training and validation\n",
    "        max_epochs: Maximum number of epochs for training\n",
    "        patience: Number of epochs to wait with early stopping the\n",
    "                  training if validation loss has decreased\n",
    "                  \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_acc = 0\n",
    "    curr_patience = 0\n",
    "    #print(\"fit running\")\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        #print(\"going to run epoch\")\n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
    "        \n",
    "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
    "        \n",
    "        # TODO: Add early stopping and save the best weights (in best_model_weights)\n",
    "        \n",
    "        if best_acc == 0 or val_acc > best_acc :\n",
    "            val_best_acc = val_acc\n",
    "            curr_patience = 0\n",
    "            best_model_weights = copy.deepcopy(model.state_dict)\n",
    "        else :\n",
    "            current_patience += 1\n",
    "            if current_patience >= patience :\n",
    "                print(\"Stopping early at epoch {}!\".format(epoch+1))\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases you should just use the Adam optimizer for training, because it works well out of the box. However, a well-tuned SGD (with momentum) will in most cases outperform Adam. And since the original paper gives us a well-tuned SGD we will just use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "loss and accuracy at minibatch 0!\n",
      "3.866533041000366\n",
      "12.0\n",
      "loss and accuracy at minibatch 1!\n",
      "6.891152381896973\n",
      "28.0\n",
      "loss and accuracy at minibatch 2!\n",
      "10.597957372665405\n",
      "51.0\n",
      "loss and accuracy at minibatch 3!\n",
      "14.207668781280518\n",
      "63.0\n",
      "loss and accuracy at minibatch 4!\n",
      "18.11723303794861\n",
      "79.0\n",
      "loss and accuracy at minibatch 5!\n",
      "21.551703453063965\n",
      "104.0\n",
      "loss and accuracy at minibatch 6!\n",
      "25.885797023773193\n",
      "123.0\n",
      "loss and accuracy at minibatch 7!\n",
      "29.56934380531311\n",
      "137.0\n",
      "loss and accuracy at minibatch 8!\n",
      "32.759204149246216\n",
      "153.0\n",
      "loss and accuracy at minibatch 9!\n",
      "35.691073417663574\n",
      "177.0\n",
      "loss and accuracy at minibatch 10!\n",
      "39.27406287193298\n",
      "206.0\n",
      "loss and accuracy at minibatch 11!\n",
      "42.279764890670776\n",
      "227.0\n",
      "loss and accuracy at minibatch 12!\n",
      "45.0328094959259\n",
      "253.0\n",
      "loss and accuracy at minibatch 13!\n",
      "47.97999691963196\n",
      "278.0\n",
      "loss and accuracy at minibatch 14!\n",
      "50.452752351760864\n",
      "304.0\n",
      "loss and accuracy at minibatch 15!\n",
      "53.3086142539978\n",
      "333.0\n",
      "loss and accuracy at minibatch 16!\n",
      "55.899131536483765\n",
      "360.0\n",
      "loss and accuracy at minibatch 17!\n",
      "58.56710243225098\n",
      "388.0\n",
      "loss and accuracy at minibatch 18!\n",
      "61.203370571136475\n",
      "406.0\n",
      "loss and accuracy at minibatch 19!\n",
      "64.09281349182129\n",
      "421.0\n",
      "loss and accuracy at minibatch 20!\n",
      "66.91713619232178\n",
      "434.0\n",
      "loss and accuracy at minibatch 21!\n",
      "70.00873064994812\n",
      "456.0\n",
      "loss and accuracy at minibatch 22!\n",
      "72.6844642162323\n",
      "469.0\n",
      "loss and accuracy at minibatch 23!\n",
      "75.10298228263855\n",
      "483.0\n",
      "loss and accuracy at minibatch 24!\n",
      "77.57216453552246\n",
      "497.0\n",
      "loss and accuracy at minibatch 25!\n",
      "79.84894299507141\n",
      "516.0\n",
      "loss and accuracy at minibatch 26!\n",
      "82.42756009101868\n",
      "537.0\n",
      "loss and accuracy at minibatch 27!\n",
      "84.72116899490356\n",
      "557.0\n",
      "loss and accuracy at minibatch 28!\n",
      "86.91330027580261\n",
      "587.0\n",
      "loss and accuracy at minibatch 29!\n",
      "89.22903871536255\n",
      "611.0\n",
      "loss and accuracy at minibatch 30!\n",
      "91.60985231399536\n",
      "631.0\n",
      "loss and accuracy at minibatch 31!\n",
      "93.73024797439575\n",
      "657.0\n",
      "loss and accuracy at minibatch 32!\n",
      "95.97956490516663\n",
      "681.0\n",
      "loss and accuracy at minibatch 33!\n",
      "98.12744188308716\n",
      "703.0\n",
      "loss and accuracy at minibatch 34!\n",
      "100.26561617851257\n",
      "730.0\n",
      "loss and accuracy at minibatch 35!\n",
      "102.34546828269958\n",
      "760.0\n",
      "loss and accuracy at minibatch 36!\n",
      "104.54686760902405\n",
      "781.0\n",
      "loss and accuracy at minibatch 37!\n",
      "106.9660279750824\n",
      "810.0\n",
      "loss and accuracy at minibatch 38!\n",
      "109.06900119781494\n",
      "837.0\n",
      "loss and accuracy at minibatch 39!\n",
      "111.35488486289978\n",
      "860.0\n",
      "loss and accuracy at minibatch 40!\n",
      "113.38029193878174\n",
      "889.0\n",
      "loss and accuracy at minibatch 41!\n",
      "115.45034074783325\n",
      "917.0\n",
      "loss and accuracy at minibatch 42!\n",
      "117.59964203834534\n",
      "944.0\n",
      "loss and accuracy at minibatch 43!\n",
      "119.88368248939514\n",
      "965.0\n",
      "loss and accuracy at minibatch 44!\n",
      "121.89673519134521\n",
      "996.0\n",
      "loss and accuracy at minibatch 45!\n",
      "124.00586175918579\n",
      "1019.0\n",
      "loss and accuracy at minibatch 46!\n",
      "126.22108101844788\n",
      "1035.0\n",
      "loss and accuracy at minibatch 47!\n",
      "128.36374926567078\n",
      "1059.0\n",
      "loss and accuracy at minibatch 48!\n",
      "130.53774094581604\n",
      "1087.0\n",
      "loss and accuracy at minibatch 49!\n",
      "132.62348294258118\n",
      "1111.0\n",
      "loss and accuracy at minibatch 50!\n",
      "134.70570993423462\n",
      "1135.0\n",
      "loss and accuracy at minibatch 51!\n",
      "136.7248558998108\n",
      "1160.0\n",
      "loss and accuracy at minibatch 52!\n",
      "138.85459280014038\n",
      "1188.0\n",
      "loss and accuracy at minibatch 53!\n",
      "141.05090951919556\n",
      "1204.0\n",
      "loss and accuracy at minibatch 54!\n",
      "143.04712533950806\n",
      "1234.0\n",
      "loss and accuracy at minibatch 55!\n",
      "145.09294533729553\n",
      "1263.0\n",
      "loss and accuracy at minibatch 56!\n",
      "147.10716557502747\n",
      "1293.0\n",
      "loss and accuracy at minibatch 57!\n",
      "149.0539187192917\n",
      "1328.0\n",
      "loss and accuracy at minibatch 58!\n",
      "150.9963343143463\n",
      "1363.0\n",
      "loss and accuracy at minibatch 59!\n",
      "152.8689647912979\n",
      "1395.0\n",
      "loss and accuracy at minibatch 60!\n",
      "154.82901632785797\n",
      "1426.0\n",
      "loss and accuracy at minibatch 61!\n",
      "156.698135972023\n",
      "1464.0\n",
      "loss and accuracy at minibatch 62!\n",
      "158.7322188615799\n",
      "1490.0\n",
      "loss and accuracy at minibatch 63!\n",
      "160.69293630123138\n",
      "1528.0\n",
      "loss and accuracy at minibatch 64!\n",
      "162.7357405424118\n",
      "1564.0\n",
      "loss and accuracy at minibatch 65!\n",
      "164.66987550258636\n",
      "1595.0\n",
      "loss and accuracy at minibatch 66!\n",
      "166.71168792247772\n",
      "1627.0\n",
      "loss and accuracy at minibatch 67!\n",
      "168.8061307668686\n",
      "1656.0\n",
      "loss and accuracy at minibatch 68!\n",
      "170.71522498130798\n",
      "1688.0\n",
      "loss and accuracy at minibatch 69!\n",
      "172.682630777359\n",
      "1723.0\n",
      "loss and accuracy at minibatch 70!\n",
      "174.58032417297363\n",
      "1761.0\n",
      "loss and accuracy at minibatch 71!\n",
      "176.56920862197876\n",
      "1789.0\n",
      "loss and accuracy at minibatch 72!\n",
      "178.53409612178802\n",
      "1826.0\n",
      "loss and accuracy at minibatch 73!\n",
      "180.5042655467987\n",
      "1861.0\n",
      "loss and accuracy at minibatch 74!\n",
      "182.38190805912018\n",
      "1894.0\n",
      "loss and accuracy at minibatch 75!\n",
      "184.35863602161407\n",
      "1928.0\n",
      "loss and accuracy at minibatch 76!\n",
      "186.33194184303284\n",
      "1960.0\n",
      "loss and accuracy at minibatch 77!\n",
      "188.30240082740784\n",
      "1995.0\n",
      "loss and accuracy at minibatch 78!\n",
      "190.18578732013702\n",
      "2031.0\n",
      "loss and accuracy at minibatch 79!\n",
      "192.1676688194275\n",
      "2063.0\n",
      "loss and accuracy at minibatch 80!\n",
      "194.03837859630585\n",
      "2098.0\n",
      "loss and accuracy at minibatch 81!\n",
      "195.93766951560974\n",
      "2134.0\n",
      "loss and accuracy at minibatch 82!\n",
      "197.8760985136032\n",
      "2165.0\n",
      "loss and accuracy at minibatch 83!\n",
      "199.71258509159088\n",
      "2209.0\n",
      "loss and accuracy at minibatch 84!\n",
      "201.6580536365509\n",
      "2249.0\n",
      "loss and accuracy at minibatch 85!\n",
      "203.5403745174408\n",
      "2286.0\n",
      "loss and accuracy at minibatch 86!\n",
      "205.4067256450653\n",
      "2321.0\n",
      "loss and accuracy at minibatch 87!\n",
      "207.22941958904266\n",
      "2358.0\n",
      "loss and accuracy at minibatch 88!\n",
      "209.21202540397644\n",
      "2383.0\n",
      "loss and accuracy at minibatch 89!\n",
      "211.17929029464722\n",
      "2426.0\n",
      "loss and accuracy at minibatch 90!\n",
      "213.1147345304489\n",
      "2458.0\n",
      "loss and accuracy at minibatch 91!\n",
      "215.12137115001678\n",
      "2491.0\n",
      "loss and accuracy at minibatch 92!\n",
      "217.15781843662262\n",
      "2518.0\n",
      "loss and accuracy at minibatch 93!\n",
      "219.08573830127716\n",
      "2555.0\n",
      "loss and accuracy at minibatch 94!\n",
      "221.02191603183746\n",
      "2590.0\n",
      "loss and accuracy at minibatch 95!\n",
      "222.8559856414795\n",
      "2625.0\n",
      "loss and accuracy at minibatch 96!\n",
      "224.7285875082016\n",
      "2669.0\n",
      "loss and accuracy at minibatch 97!\n",
      "226.60427951812744\n",
      "2697.0\n",
      "loss and accuracy at minibatch 98!\n",
      "228.4470535516739\n",
      "2729.0\n",
      "loss and accuracy at minibatch 99!\n",
      "230.28539562225342\n",
      "2767.0\n",
      "loss and accuracy at minibatch 100!\n",
      "232.28990650177002\n",
      "2805.0\n",
      "loss and accuracy at minibatch 101!\n",
      "234.14384579658508\n",
      "2837.0\n",
      "loss and accuracy at minibatch 102!\n",
      "236.09655714035034\n",
      "2875.0\n",
      "loss and accuracy at minibatch 103!\n",
      "238.02539372444153\n",
      "2919.0\n",
      "loss and accuracy at minibatch 104!\n",
      "239.97911608219147\n",
      "2955.0\n",
      "loss and accuracy at minibatch 105!\n",
      "241.86943018436432\n",
      "2992.0\n",
      "loss and accuracy at minibatch 106!\n",
      "243.7587298154831\n",
      "3028.0\n",
      "loss and accuracy at minibatch 107!\n",
      "245.561478972435\n",
      "3065.0\n",
      "loss and accuracy at minibatch 108!\n",
      "247.34517073631287\n",
      "3095.0\n",
      "loss and accuracy at minibatch 109!\n",
      "249.1672625541687\n",
      "3130.0\n",
      "loss and accuracy at minibatch 110!\n",
      "251.07361662387848\n",
      "3159.0\n",
      "loss and accuracy at minibatch 111!\n",
      "252.96522665023804\n",
      "3189.0\n",
      "loss and accuracy at minibatch 112!\n",
      "254.79468953609467\n",
      "3219.0\n",
      "loss and accuracy at minibatch 113!\n",
      "256.6876940727234\n",
      "3258.0\n",
      "loss and accuracy at minibatch 114!\n",
      "258.5633511543274\n",
      "3295.0\n",
      "loss and accuracy at minibatch 115!\n",
      "260.37225019931793\n",
      "3342.0\n",
      "loss and accuracy at minibatch 116!\n",
      "262.39710533618927\n",
      "3377.0\n",
      "loss and accuracy at minibatch 117!\n",
      "264.4047623872757\n",
      "3408.0\n",
      "loss and accuracy at minibatch 118!\n",
      "266.24303019046783\n",
      "3444.0\n",
      "loss and accuracy at minibatch 119!\n",
      "268.0000638961792\n",
      "3489.0\n",
      "loss and accuracy at minibatch 120!\n",
      "269.6598148345947\n",
      "3541.0\n",
      "loss and accuracy at minibatch 121!\n",
      "271.64009618759155\n",
      "3576.0\n",
      "loss and accuracy at minibatch 122!\n",
      "273.50682282447815\n",
      "3615.0\n",
      "loss and accuracy at minibatch 123!\n",
      "275.33605802059174\n",
      "3653.0\n",
      "loss and accuracy at minibatch 124!\n",
      "277.1631290912628\n",
      "3689.0\n",
      "loss and accuracy at minibatch 125!\n",
      "278.8934882879257\n",
      "3738.0\n",
      "loss and accuracy at minibatch 126!\n",
      "280.7479872703552\n",
      "3781.0\n",
      "loss and accuracy at minibatch 127!\n",
      "282.7342857122421\n",
      "3818.0\n",
      "loss and accuracy at minibatch 128!\n",
      "284.6558747291565\n",
      "3847.0\n",
      "loss and accuracy at minibatch 129!\n",
      "286.4915589094162\n",
      "3878.0\n",
      "loss and accuracy at minibatch 130!\n",
      "288.32442808151245\n",
      "3921.0\n",
      "loss and accuracy at minibatch 131!\n",
      "290.14060842990875\n",
      "3960.0\n",
      "loss and accuracy at minibatch 132!\n",
      "291.96055567264557\n",
      "3997.0\n",
      "loss and accuracy at minibatch 133!\n",
      "293.8455790281296\n",
      "4030.0\n",
      "loss and accuracy at minibatch 134!\n",
      "295.7232881784439\n",
      "4066.0\n",
      "loss and accuracy at minibatch 135!\n",
      "297.5544056892395\n",
      "4103.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss and accuracy at minibatch 136!\n",
      "299.342728972435\n",
      "4143.0\n",
      "loss and accuracy at minibatch 137!\n",
      "301.07266116142273\n",
      "4183.0\n",
      "loss and accuracy at minibatch 138!\n",
      "302.90768480300903\n",
      "4222.0\n",
      "loss and accuracy at minibatch 139!\n",
      "304.74943137168884\n",
      "4261.0\n",
      "loss and accuracy at minibatch 140!\n",
      "306.5044445991516\n",
      "4298.0\n",
      "loss and accuracy at minibatch 141!\n",
      "308.1657314300537\n",
      "4343.0\n",
      "loss and accuracy at minibatch 142!\n",
      "310.0850336551666\n",
      "4375.0\n",
      "loss and accuracy at minibatch 143!\n",
      "311.8767387866974\n",
      "4417.0\n",
      "loss and accuracy at minibatch 144!\n",
      "313.5237271785736\n",
      "4461.0\n",
      "loss and accuracy at minibatch 145!\n",
      "315.2228432893753\n",
      "4502.0\n",
      "loss and accuracy at minibatch 146!\n",
      "316.95178496837616\n",
      "4544.0\n",
      "loss and accuracy at minibatch 147!\n",
      "318.6015702486038\n",
      "4587.0\n",
      "loss and accuracy at minibatch 148!\n",
      "320.35326731204987\n",
      "4633.0\n",
      "loss and accuracy at minibatch 149!\n",
      "322.3445975780487\n",
      "4669.0\n",
      "loss and accuracy at minibatch 150!\n",
      "323.97682082653046\n",
      "4714.0\n",
      "loss and accuracy at minibatch 151!\n",
      "325.70947432518005\n",
      "4751.0\n",
      "loss and accuracy at minibatch 152!\n",
      "327.67920899391174\n",
      "4788.0\n",
      "loss and accuracy at minibatch 153!\n",
      "329.52156591415405\n",
      "4823.0\n",
      "loss and accuracy at minibatch 154!\n",
      "331.399561047554\n",
      "4865.0\n",
      "loss and accuracy at minibatch 155!\n",
      "333.2020570039749\n",
      "4902.0\n",
      "loss and accuracy at minibatch 156!\n",
      "334.94856309890747\n",
      "4940.0\n",
      "loss and accuracy at minibatch 157!\n",
      "336.6679948568344\n",
      "4985.0\n",
      "loss and accuracy at minibatch 158!\n",
      "338.555646777153\n",
      "5016.0\n",
      "loss and accuracy at minibatch 159!\n",
      "340.3598051071167\n",
      "5058.0\n",
      "loss and accuracy at minibatch 160!\n",
      "342.14291536808014\n",
      "5092.0\n",
      "loss and accuracy at minibatch 161!\n",
      "343.9793344736099\n",
      "5131.0\n",
      "loss and accuracy at minibatch 162!\n",
      "345.6608941555023\n",
      "5187.0\n",
      "loss and accuracy at minibatch 163!\n",
      "347.46464240550995\n",
      "5220.0\n",
      "loss and accuracy at minibatch 164!\n",
      "349.26899087429047\n",
      "5258.0\n",
      "loss and accuracy at minibatch 165!\n",
      "351.06623780727386\n",
      "5301.0\n",
      "loss and accuracy at minibatch 166!\n",
      "352.79353964328766\n",
      "5349.0\n",
      "loss and accuracy at minibatch 167!\n",
      "354.47779536247253\n",
      "5395.0\n",
      "loss and accuracy at minibatch 168!\n",
      "356.3260108232498\n",
      "5434.0\n",
      "loss and accuracy at minibatch 169!\n",
      "358.14042115211487\n",
      "5482.0\n",
      "loss and accuracy at minibatch 170!\n",
      "359.87793588638306\n",
      "5529.0\n",
      "loss and accuracy at minibatch 171!\n",
      "361.6872787475586\n",
      "5565.0\n",
      "loss and accuracy at minibatch 172!\n",
      "363.5773169994354\n",
      "5601.0\n",
      "loss and accuracy at minibatch 173!\n",
      "365.3060305118561\n",
      "5648.0\n",
      "loss and accuracy at minibatch 174!\n",
      "367.06594038009644\n",
      "5683.0\n",
      "loss and accuracy at minibatch 175!\n",
      "368.94165313243866\n",
      "5720.0\n",
      "loss and accuracy at minibatch 176!\n",
      "370.57557821273804\n",
      "5772.0\n",
      "loss and accuracy at minibatch 177!\n",
      "372.4577680826187\n",
      "5809.0\n",
      "loss and accuracy at minibatch 178!\n",
      "374.2020491361618\n",
      "5855.0\n",
      "loss and accuracy at minibatch 179!\n",
      "375.96895718574524\n",
      "5898.0\n",
      "loss and accuracy at minibatch 180!\n",
      "377.80568075180054\n",
      "5945.0\n",
      "loss and accuracy at minibatch 181!\n",
      "379.44901907444\n",
      "5993.0\n",
      "loss and accuracy at minibatch 182!\n",
      "381.1006965637207\n",
      "6041.0\n",
      "loss and accuracy at minibatch 183!\n",
      "382.7029116153717\n",
      "6093.0\n",
      "loss and accuracy at minibatch 184!\n",
      "384.33710169792175\n",
      "6144.0\n",
      "loss and accuracy at minibatch 185!\n",
      "386.01396095752716\n",
      "6191.0\n",
      "loss and accuracy at minibatch 186!\n",
      "387.5553300380707\n",
      "6242.0\n",
      "loss and accuracy at minibatch 187!\n",
      "389.358274102211\n",
      "6283.0\n",
      "loss and accuracy at minibatch 188!\n",
      "391.1893072128296\n",
      "6330.0\n",
      "loss and accuracy at minibatch 189!\n",
      "392.73761677742004\n",
      "6389.0\n",
      "loss and accuracy at minibatch 190!\n",
      "394.6210209131241\n",
      "6432.0\n",
      "loss and accuracy at minibatch 191!\n",
      "396.3256789445877\n",
      "6475.0\n",
      "loss and accuracy at minibatch 192!\n",
      "398.1628484725952\n",
      "6521.0\n",
      "loss and accuracy at minibatch 193!\n",
      "399.97069561481476\n",
      "6561.0\n",
      "loss and accuracy at minibatch 194!\n",
      "401.7824898958206\n",
      "6593.0\n",
      "loss and accuracy at minibatch 195!\n",
      "403.47386622428894\n",
      "6637.0\n",
      "loss and accuracy at minibatch 196!\n",
      "405.1309324502945\n",
      "6689.0\n",
      "loss and accuracy at minibatch 197!\n",
      "406.7950817346573\n",
      "6741.0\n",
      "loss and accuracy at minibatch 198!\n",
      "408.4292793273926\n",
      "6785.0\n",
      "loss and accuracy at minibatch 199!\n",
      "410.1672066450119\n",
      "6830.0\n",
      "loss and accuracy at minibatch 200!\n",
      "411.98832952976227\n",
      "6869.0\n",
      "loss and accuracy at minibatch 201!\n",
      "413.77247726917267\n",
      "6915.0\n",
      "loss and accuracy at minibatch 202!\n",
      "415.42364621162415\n",
      "6965.0\n",
      "loss and accuracy at minibatch 203!\n",
      "417.184023976326\n",
      "7009.0\n",
      "loss and accuracy at minibatch 204!\n",
      "418.8006069660187\n",
      "7057.0\n",
      "loss and accuracy at minibatch 205!\n",
      "420.4715647697449\n",
      "7105.0\n",
      "loss and accuracy at minibatch 206!\n",
      "422.03479051589966\n",
      "7152.0\n",
      "loss and accuracy at minibatch 207!\n",
      "423.7482045888901\n",
      "7207.0\n",
      "loss and accuracy at minibatch 208!\n",
      "425.26628971099854\n",
      "7262.0\n",
      "loss and accuracy at minibatch 209!\n",
      "426.9132659435272\n",
      "7311.0\n",
      "loss and accuracy at minibatch 210!\n",
      "428.54706716537476\n",
      "7359.0\n",
      "loss and accuracy at minibatch 211!\n",
      "430.08673799037933\n",
      "7421.0\n",
      "loss and accuracy at minibatch 212!\n",
      "431.7732262611389\n",
      "7472.0\n",
      "loss and accuracy at minibatch 213!\n",
      "433.4472836256027\n",
      "7522.0\n",
      "loss and accuracy at minibatch 214!\n",
      "434.9629638195038\n",
      "7576.0\n",
      "loss and accuracy at minibatch 215!\n",
      "436.7242749929428\n",
      "7628.0\n",
      "loss and accuracy at minibatch 216!\n",
      "438.46111238002777\n",
      "7666.0\n",
      "loss and accuracy at minibatch 217!\n",
      "440.1983597278595\n",
      "7711.0\n",
      "loss and accuracy at minibatch 218!\n",
      "442.0273973941803\n",
      "7751.0\n",
      "loss and accuracy at minibatch 219!\n",
      "443.89097583293915\n",
      "7790.0\n",
      "loss and accuracy at minibatch 220!\n",
      "445.5397781133652\n",
      "7833.0\n",
      "loss and accuracy at minibatch 221!\n",
      "447.20677161216736\n",
      "7878.0\n",
      "loss and accuracy at minibatch 222!\n",
      "448.73799455165863\n",
      "7933.0\n",
      "loss and accuracy at minibatch 223!\n",
      "450.47969591617584\n",
      "7984.0\n",
      "loss and accuracy at minibatch 224!\n",
      "452.243745803833\n",
      "8031.0\n",
      "loss and accuracy at minibatch 225!\n",
      "454.01175034046173\n",
      "8071.0\n",
      "loss and accuracy at minibatch 226!\n",
      "455.8801212310791\n",
      "8111.0\n",
      "loss and accuracy at minibatch 227!\n",
      "457.56288027763367\n",
      "8158.0\n",
      "loss and accuracy at minibatch 228!\n",
      "459.20373940467834\n",
      "8207.0\n",
      "loss and accuracy at minibatch 229!\n",
      "460.9224878549576\n",
      "8259.0\n",
      "loss and accuracy at minibatch 230!\n",
      "462.72894620895386\n",
      "8297.0\n",
      "loss and accuracy at minibatch 231!\n",
      "464.33545756340027\n",
      "8347.0\n",
      "loss and accuracy at minibatch 232!\n",
      "466.1049373149872\n",
      "8386.0\n",
      "loss and accuracy at minibatch 233!\n",
      "467.7818548679352\n",
      "8428.0\n",
      "loss and accuracy at minibatch 234!\n",
      "469.44904458522797\n",
      "8478.0\n",
      "loss and accuracy at minibatch 235!\n",
      "471.0969752073288\n",
      "8521.0\n",
      "loss and accuracy at minibatch 236!\n",
      "472.82482147216797\n",
      "8569.0\n",
      "loss and accuracy at minibatch 237!\n",
      "474.50507378578186\n",
      "8617.0\n",
      "loss and accuracy at minibatch 238!\n",
      "476.1749460697174\n",
      "8664.0\n",
      "loss and accuracy at minibatch 239!\n",
      "477.8639667034149\n",
      "8711.0\n",
      "loss and accuracy at minibatch 240!\n",
      "479.56403732299805\n",
      "8758.0\n",
      "loss and accuracy at minibatch 241!\n",
      "481.24703216552734\n",
      "8809.0\n",
      "loss and accuracy at minibatch 242!\n",
      "482.871768116951\n",
      "8859.0\n",
      "loss and accuracy at minibatch 243!\n",
      "484.5272401571274\n",
      "8910.0\n",
      "loss and accuracy at minibatch 244!\n",
      "486.21848690509796\n",
      "8958.0\n",
      "loss and accuracy at minibatch 245!\n",
      "487.8503186702728\n",
      "9011.0\n",
      "loss and accuracy at minibatch 246!\n",
      "489.5245872735977\n",
      "9061.0\n",
      "loss and accuracy at minibatch 247!\n",
      "491.04558420181274\n",
      "9119.0\n",
      "loss and accuracy at minibatch 248!\n",
      "492.65818643569946\n",
      "9169.0\n",
      "loss and accuracy at minibatch 249!\n",
      "494.41116416454315\n",
      "9209.0\n",
      "loss and accuracy at minibatch 250!\n",
      "495.9791147708893\n",
      "9258.0\n",
      "loss and accuracy at minibatch 251!\n",
      "497.75544917583466\n",
      "9308.0\n",
      "loss and accuracy at minibatch 252!\n",
      "499.40414321422577\n",
      "9360.0\n",
      "loss and accuracy at minibatch 253!\n",
      "501.0367588996887\n",
      "9413.0\n",
      "loss and accuracy at minibatch 254!\n",
      "502.7549886703491\n",
      "9456.0\n",
      "loss and accuracy at minibatch 255!\n",
      "504.34374248981476\n",
      "9511.0\n",
      "loss and accuracy at minibatch 256!\n",
      "506.06052899360657\n",
      "9557.0\n",
      "loss and accuracy at minibatch 257!\n",
      "507.6145907640457\n",
      "9606.0\n",
      "loss and accuracy at minibatch 258!\n",
      "509.14159309864044\n",
      "9660.0\n",
      "loss and accuracy at minibatch 259!\n",
      "510.6959698200226\n",
      "9719.0\n",
      "loss and accuracy at minibatch 260!\n",
      "512.1831291913986\n",
      "9775.0\n",
      "loss and accuracy at minibatch 261!\n",
      "513.774783372879\n",
      "9822.0\n",
      "loss and accuracy at minibatch 262!\n",
      "515.3622957468033\n",
      "9872.0\n",
      "loss and accuracy at minibatch 263!\n",
      "516.9944934844971\n",
      "9917.0\n",
      "loss and accuracy at minibatch 264!\n",
      "518.8003433942795\n",
      "9964.0\n",
      "loss and accuracy at minibatch 265!\n",
      "520.3345136642456\n",
      "10016.0\n",
      "loss and accuracy at minibatch 266!\n",
      "521.8638405799866\n",
      "10064.0\n",
      "loss and accuracy at minibatch 267!\n",
      "523.632285952568\n",
      "10117.0\n",
      "loss and accuracy at minibatch 268!\n",
      "525.0885342359543\n",
      "10174.0\n",
      "loss and accuracy at minibatch 269!\n",
      "526.7575365304947\n",
      "10223.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss and accuracy at minibatch 270!\n",
      "528.2391542196274\n",
      "10276.0\n",
      "loss and accuracy at minibatch 271!\n",
      "529.7082674503326\n",
      "10334.0\n",
      "loss and accuracy at minibatch 272!\n",
      "531.1488308906555\n",
      "10396.0\n",
      "loss and accuracy at minibatch 273!\n",
      "532.9072924852371\n",
      "10440.0\n",
      "loss and accuracy at minibatch 274!\n",
      "534.4933171272278\n",
      "10484.0\n",
      "loss and accuracy at minibatch 275!\n",
      "535.9869232177734\n",
      "10540.0\n",
      "loss and accuracy at minibatch 276!\n",
      "537.6736345291138\n",
      "10586.0\n",
      "loss and accuracy at minibatch 277!\n",
      "539.2855883836746\n",
      "10641.0\n",
      "loss and accuracy at minibatch 278!\n",
      "540.7895117998123\n",
      "10695.0\n",
      "loss and accuracy at minibatch 279!\n",
      "542.3762695789337\n",
      "10745.0\n",
      "loss and accuracy at minibatch 280!\n",
      "543.8896112442017\n",
      "10799.0\n",
      "loss and accuracy at minibatch 281!\n",
      "545.3737144470215\n",
      "10849.0\n",
      "loss and accuracy at minibatch 282!\n",
      "546.9414956569672\n",
      "10907.0\n",
      "loss and accuracy at minibatch 283!\n",
      "548.5580812692642\n",
      "10959.0\n",
      "loss and accuracy at minibatch 284!\n",
      "549.999298453331\n",
      "11021.0\n",
      "loss and accuracy at minibatch 285!\n",
      "551.587813615799\n",
      "11074.0\n",
      "loss and accuracy at minibatch 286!\n",
      "553.1692284345627\n",
      "11133.0\n",
      "loss and accuracy at minibatch 287!\n",
      "554.7256915569305\n",
      "11185.0\n",
      "loss and accuracy at minibatch 288!\n",
      "556.1918709278107\n",
      "11239.0\n",
      "loss and accuracy at minibatch 289!\n",
      "557.810002207756\n",
      "11284.0\n",
      "loss and accuracy at minibatch 290!\n",
      "559.341269493103\n",
      "11340.0\n",
      "loss and accuracy at minibatch 291!\n",
      "560.9165550470352\n",
      "11383.0\n",
      "loss and accuracy at minibatch 292!\n",
      "562.5323847532272\n",
      "11436.0\n",
      "loss and accuracy at minibatch 293!\n",
      "564.1188117265701\n",
      "11491.0\n",
      "loss and accuracy at minibatch 294!\n",
      "565.5102552175522\n",
      "11557.0\n",
      "loss and accuracy at minibatch 295!\n",
      "566.9223296642303\n",
      "11622.0\n",
      "loss and accuracy at minibatch 296!\n",
      "568.3821719884872\n",
      "11678.0\n",
      "loss and accuracy at minibatch 297!\n",
      "570.0364310741425\n",
      "11726.0\n",
      "loss and accuracy at minibatch 298!\n",
      "571.4591360092163\n",
      "11791.0\n",
      "loss and accuracy at minibatch 299!\n",
      "572.9157067537308\n",
      "11850.0\n",
      "loss and accuracy at minibatch 300!\n",
      "574.5025668144226\n",
      "11903.0\n",
      "loss and accuracy at minibatch 301!\n",
      "576.1294393539429\n",
      "11952.0\n",
      "loss and accuracy at minibatch 302!\n",
      "577.8215217590332\n",
      "11995.0\n",
      "loss and accuracy at minibatch 303!\n",
      "579.3290098905563\n",
      "12048.0\n",
      "loss and accuracy at minibatch 304!\n",
      "581.007670879364\n",
      "12094.0\n",
      "loss and accuracy at minibatch 305!\n",
      "582.6243326663971\n",
      "12146.0\n",
      "loss and accuracy at minibatch 306!\n",
      "584.0700306892395\n",
      "12204.0\n",
      "loss and accuracy at minibatch 307!\n",
      "585.5188380479813\n",
      "12261.0\n",
      "loss and accuracy at minibatch 308!\n",
      "586.9768022298813\n",
      "12317.0\n",
      "loss and accuracy at minibatch 309!\n",
      "588.5081448554993\n",
      "12369.0\n",
      "loss and accuracy at minibatch 310!\n",
      "590.0429584980011\n",
      "12428.0\n",
      "loss and accuracy at minibatch 311!\n",
      "591.7877526283264\n",
      "12471.0\n",
      "loss and accuracy at minibatch 312!\n",
      "593.3184769153595\n",
      "12534.0\n",
      "loss and accuracy at minibatch 313!\n",
      "594.9692468643188\n",
      "12584.0\n",
      "loss and accuracy at minibatch 314!\n",
      "596.4055080413818\n",
      "12644.0\n",
      "loss and accuracy at minibatch 315!\n",
      "597.8726987838745\n",
      "12700.0\n",
      "loss and accuracy at minibatch 316!\n",
      "599.329088807106\n",
      "12757.0\n",
      "loss and accuracy at minibatch 317!\n",
      "600.7257851362228\n",
      "12823.0\n",
      "loss and accuracy at minibatch 318!\n",
      "602.176727771759\n",
      "12887.0\n",
      "loss and accuracy at minibatch 319!\n",
      "603.6562532186508\n",
      "12941.0\n",
      "loss and accuracy at minibatch 320!\n",
      "605.0958939790726\n",
      "12996.0\n",
      "loss and accuracy at minibatch 321!\n",
      "606.6019929647446\n",
      "13060.0\n",
      "loss and accuracy at minibatch 322!\n",
      "608.0861167907715\n",
      "13116.0\n",
      "loss and accuracy at minibatch 323!\n",
      "609.5186108350754\n",
      "13178.0\n",
      "loss and accuracy at minibatch 324!\n",
      "611.0644311904907\n",
      "13231.0\n",
      "loss and accuracy at minibatch 325!\n",
      "612.7165526151657\n",
      "13286.0\n",
      "loss and accuracy at minibatch 326!\n",
      "614.1242398023605\n",
      "13345.0\n",
      "loss and accuracy at minibatch 327!\n",
      "615.5746853351593\n",
      "13402.0\n",
      "loss and accuracy at minibatch 328!\n",
      "617.369281411171\n",
      "13449.0\n",
      "loss and accuracy at minibatch 329!\n",
      "618.8178405761719\n",
      "13512.0\n",
      "loss and accuracy at minibatch 330!\n",
      "620.445837020874\n",
      "13566.0\n",
      "loss and accuracy at minibatch 331!\n",
      "621.7887511253357\n",
      "13627.0\n",
      "loss and accuracy at minibatch 332!\n",
      "623.265621304512\n",
      "13680.0\n",
      "loss and accuracy at minibatch 333!\n",
      "624.8333415985107\n",
      "13737.0\n",
      "loss and accuracy at minibatch 334!\n",
      "626.2840754985809\n",
      "13797.0\n",
      "loss and accuracy at minibatch 335!\n",
      "627.6968758106232\n",
      "13851.0\n",
      "loss and accuracy at minibatch 336!\n",
      "629.2680633068085\n",
      "13905.0\n",
      "loss and accuracy at minibatch 337!\n",
      "630.6440057754517\n",
      "13973.0\n",
      "loss and accuracy at minibatch 338!\n",
      "631.9478281736374\n",
      "14038.0\n",
      "loss and accuracy at minibatch 339!\n",
      "633.2214860916138\n",
      "14116.0\n",
      "loss and accuracy at minibatch 340!\n",
      "634.6314125061035\n",
      "14186.0\n",
      "loss and accuracy at minibatch 341!\n",
      "635.9913750886917\n",
      "14253.0\n",
      "loss and accuracy at minibatch 342!\n",
      "637.5431958436966\n",
      "14315.0\n",
      "loss and accuracy at minibatch 343!\n",
      "638.8609350919724\n",
      "14374.0\n",
      "loss and accuracy at minibatch 344!\n",
      "640.3055064678192\n",
      "14429.0\n",
      "loss and accuracy at minibatch 345!\n",
      "641.8806796073914\n",
      "14482.0\n",
      "loss and accuracy at minibatch 346!\n",
      "643.4341235160828\n",
      "14540.0\n",
      "loss and accuracy at minibatch 347!\n",
      "644.9488047361374\n",
      "14600.0\n",
      "loss and accuracy at minibatch 348!\n",
      "646.5134571790695\n",
      "14652.0\n",
      "loss and accuracy at minibatch 349!\n",
      "648.1349313259125\n",
      "14709.0\n",
      "loss and accuracy at minibatch 350!\n",
      "649.5767306089401\n",
      "14760.0\n",
      "loss and accuracy at minibatch 351!\n",
      "651.1014989614487\n",
      "14788.0\n",
      "Epoch   1/200, train loss: 1.45e-02, accuracy: 32.86%\n",
      "Epoch   1/200, val loss: 2.87e-02, accuracy: 11.22%\n",
      "loss and accuracy at minibatch 0!\n",
      "1.497278094291687\n",
      "60.0\n",
      "loss and accuracy at minibatch 1!\n",
      "3.0220497846603394\n",
      "109.0\n",
      "loss and accuracy at minibatch 2!\n",
      "4.551209330558777\n",
      "163.0\n",
      "loss and accuracy at minibatch 3!\n",
      "5.996121764183044\n",
      "223.0\n",
      "loss and accuracy at minibatch 4!\n",
      "7.6951740980148315\n",
      "268.0\n",
      "loss and accuracy at minibatch 5!\n",
      "8.986128211021423\n",
      "331.0\n",
      "loss and accuracy at minibatch 6!\n",
      "10.380137085914612\n",
      "393.0\n",
      "loss and accuracy at minibatch 7!\n",
      "11.800750017166138\n",
      "455.0\n",
      "loss and accuracy at minibatch 8!\n",
      "13.120222926139832\n",
      "521.0\n",
      "loss and accuracy at minibatch 9!\n",
      "14.63832414150238\n",
      "576.0\n",
      "loss and accuracy at minibatch 10!\n",
      "16.129348278045654\n",
      "638.0\n",
      "loss and accuracy at minibatch 11!\n",
      "17.630077004432678\n",
      "690.0\n",
      "loss and accuracy at minibatch 12!\n",
      "18.993810057640076\n",
      "748.0\n",
      "loss and accuracy at minibatch 13!\n",
      "20.363393664360046\n",
      "803.0\n",
      "loss and accuracy at minibatch 14!\n",
      "21.870335578918457\n",
      "860.0\n",
      "loss and accuracy at minibatch 15!\n",
      "23.228763937950134\n",
      "928.0\n",
      "loss and accuracy at minibatch 16!\n",
      "24.712578535079956\n",
      "982.0\n",
      "loss and accuracy at minibatch 17!\n",
      "26.16105008125305\n",
      "1040.0\n",
      "loss and accuracy at minibatch 18!\n",
      "27.695376992225647\n",
      "1095.0\n",
      "loss and accuracy at minibatch 19!\n",
      "29.11276376247406\n",
      "1156.0\n",
      "loss and accuracy at minibatch 20!\n",
      "30.517300128936768\n",
      "1220.0\n",
      "loss and accuracy at minibatch 21!\n",
      "32.01908528804779\n",
      "1279.0\n",
      "loss and accuracy at minibatch 22!\n",
      "33.3461058139801\n",
      "1343.0\n",
      "loss and accuracy at minibatch 23!\n",
      "34.90167057514191\n",
      "1399.0\n",
      "loss and accuracy at minibatch 24!\n",
      "36.415013551712036\n",
      "1451.0\n",
      "loss and accuracy at minibatch 25!\n",
      "37.752111077308655\n",
      "1516.0\n",
      "loss and accuracy at minibatch 26!\n",
      "39.101239919662476\n",
      "1579.0\n",
      "loss and accuracy at minibatch 27!\n",
      "40.57082653045654\n",
      "1635.0\n",
      "loss and accuracy at minibatch 28!\n",
      "42.006418108940125\n",
      "1694.0\n",
      "loss and accuracy at minibatch 29!\n",
      "43.41256582736969\n",
      "1755.0\n",
      "loss and accuracy at minibatch 30!\n",
      "44.79580914974213\n",
      "1812.0\n",
      "loss and accuracy at minibatch 31!\n",
      "46.37041890621185\n",
      "1865.0\n",
      "loss and accuracy at minibatch 32!\n",
      "47.753219962120056\n",
      "1926.0\n",
      "loss and accuracy at minibatch 33!\n",
      "49.25518774986267\n",
      "1987.0\n",
      "loss and accuracy at minibatch 34!\n",
      "50.71838450431824\n",
      "2048.0\n",
      "loss and accuracy at minibatch 35!\n",
      "52.18434464931488\n",
      "2106.0\n",
      "loss and accuracy at minibatch 36!\n",
      "53.4609237909317\n",
      "2177.0\n",
      "loss and accuracy at minibatch 37!\n",
      "54.78383469581604\n",
      "2238.0\n",
      "loss and accuracy at minibatch 38!\n",
      "56.19109797477722\n",
      "2296.0\n",
      "loss and accuracy at minibatch 39!\n",
      "57.61962139606476\n",
      "2359.0\n",
      "loss and accuracy at minibatch 40!\n",
      "58.95900082588196\n",
      "2421.0\n",
      "loss and accuracy at minibatch 41!\n",
      "60.146185874938965\n",
      "2497.0\n",
      "loss and accuracy at minibatch 42!\n",
      "61.507689118385315\n",
      "2555.0\n",
      "loss and accuracy at minibatch 43!\n",
      "63.05232107639313\n",
      "2611.0\n",
      "loss and accuracy at minibatch 44!\n",
      "64.37703311443329\n",
      "2681.0\n",
      "loss and accuracy at minibatch 45!\n",
      "65.83339059352875\n",
      "2746.0\n",
      "loss and accuracy at minibatch 46!\n",
      "67.44124853610992\n",
      "2807.0\n",
      "loss and accuracy at minibatch 47!\n",
      "68.83490860462189\n",
      "2878.0\n",
      "loss and accuracy at minibatch 48!\n",
      "70.35869216918945\n",
      "2925.0\n",
      "loss and accuracy at minibatch 49!\n",
      "71.60432612895966\n",
      "2997.0\n",
      "loss and accuracy at minibatch 50!\n",
      "73.00263512134552\n",
      "3059.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss and accuracy at minibatch 51!\n",
      "74.53070759773254\n",
      "3110.0\n",
      "loss and accuracy at minibatch 52!\n",
      "75.91436445713043\n",
      "3171.0\n",
      "loss and accuracy at minibatch 53!\n",
      "77.31065583229065\n",
      "3234.0\n",
      "loss and accuracy at minibatch 54!\n",
      "78.72625815868378\n",
      "3295.0\n",
      "loss and accuracy at minibatch 55!\n",
      "80.33274972438812\n",
      "3356.0\n",
      "loss and accuracy at minibatch 56!\n",
      "81.8355530500412\n",
      "3411.0\n",
      "loss and accuracy at minibatch 57!\n",
      "83.24436700344086\n",
      "3474.0\n",
      "loss and accuracy at minibatch 58!\n",
      "84.70971131324768\n",
      "3539.0\n",
      "loss and accuracy at minibatch 59!\n",
      "86.0656772851944\n",
      "3608.0\n",
      "loss and accuracy at minibatch 60!\n",
      "87.46995210647583\n",
      "3676.0\n",
      "loss and accuracy at minibatch 61!\n",
      "88.75389468669891\n",
      "3745.0\n",
      "loss and accuracy at minibatch 62!\n",
      "90.14055454730988\n",
      "3803.0\n",
      "loss and accuracy at minibatch 63!\n",
      "91.53064000606537\n",
      "3864.0\n",
      "loss and accuracy at minibatch 64!\n",
      "92.89462554454803\n",
      "3927.0\n",
      "loss and accuracy at minibatch 65!\n",
      "94.17910873889923\n",
      "3996.0\n",
      "loss and accuracy at minibatch 66!\n",
      "95.47874331474304\n",
      "4068.0\n",
      "loss and accuracy at minibatch 67!\n",
      "97.00454080104828\n",
      "4131.0\n",
      "loss and accuracy at minibatch 68!\n",
      "98.44848144054413\n",
      "4198.0\n",
      "loss and accuracy at minibatch 69!\n",
      "99.86886179447174\n",
      "4268.0\n",
      "loss and accuracy at minibatch 70!\n",
      "101.3972247838974\n",
      "4326.0\n",
      "loss and accuracy at minibatch 71!\n",
      "102.73527371883392\n",
      "4384.0\n",
      "loss and accuracy at minibatch 72!\n",
      "104.10587668418884\n",
      "4446.0\n",
      "loss and accuracy at minibatch 73!\n",
      "105.49043107032776\n",
      "4511.0\n",
      "loss and accuracy at minibatch 74!\n",
      "106.9287257194519\n",
      "4569.0\n",
      "loss and accuracy at minibatch 75!\n",
      "108.2596333026886\n",
      "4644.0\n",
      "loss and accuracy at minibatch 76!\n",
      "109.49941289424896\n",
      "4722.0\n",
      "loss and accuracy at minibatch 77!\n",
      "110.90635883808136\n",
      "4787.0\n",
      "loss and accuracy at minibatch 78!\n",
      "112.3925393819809\n",
      "4845.0\n",
      "loss and accuracy at minibatch 79!\n",
      "113.86209535598755\n",
      "4901.0\n",
      "loss and accuracy at minibatch 80!\n",
      "115.13830280303955\n",
      "4971.0\n",
      "loss and accuracy at minibatch 81!\n",
      "116.69539272785187\n",
      "5026.0\n",
      "loss and accuracy at minibatch 82!\n",
      "117.95887076854706\n",
      "5105.0\n",
      "loss and accuracy at minibatch 83!\n",
      "119.53606700897217\n",
      "5156.0\n",
      "loss and accuracy at minibatch 84!\n",
      "120.93025839328766\n",
      "5219.0\n",
      "loss and accuracy at minibatch 85!\n",
      "122.17477095127106\n",
      "5288.0\n",
      "loss and accuracy at minibatch 86!\n",
      "123.55141508579254\n",
      "5351.0\n",
      "loss and accuracy at minibatch 87!\n",
      "124.8478456735611\n",
      "5426.0\n",
      "loss and accuracy at minibatch 88!\n",
      "126.13832950592041\n",
      "5492.0\n",
      "loss and accuracy at minibatch 89!\n",
      "127.44924008846283\n",
      "5558.0\n",
      "loss and accuracy at minibatch 90!\n",
      "128.76057624816895\n",
      "5632.0\n",
      "loss and accuracy at minibatch 91!\n",
      "130.13280928134918\n",
      "5693.0\n",
      "loss and accuracy at minibatch 92!\n",
      "131.75058257579803\n",
      "5750.0\n",
      "loss and accuracy at minibatch 93!\n",
      "133.2051854133606\n",
      "5808.0\n",
      "loss and accuracy at minibatch 94!\n",
      "134.55339288711548\n",
      "5879.0\n",
      "loss and accuracy at minibatch 95!\n",
      "135.96390736103058\n",
      "5944.0\n",
      "loss and accuracy at minibatch 96!\n",
      "137.25406920909882\n",
      "6012.0\n",
      "loss and accuracy at minibatch 97!\n",
      "138.476296544075\n",
      "6084.0\n",
      "loss and accuracy at minibatch 98!\n",
      "139.7650614976883\n",
      "6153.0\n",
      "loss and accuracy at minibatch 99!\n",
      "141.0618166923523\n",
      "6219.0\n",
      "loss and accuracy at minibatch 100!\n",
      "142.43678224086761\n",
      "6283.0\n",
      "loss and accuracy at minibatch 101!\n",
      "143.74726223945618\n",
      "6350.0\n",
      "loss and accuracy at minibatch 102!\n",
      "145.26209044456482\n",
      "6409.0\n",
      "loss and accuracy at minibatch 103!\n",
      "146.8141542673111\n",
      "6468.0\n",
      "loss and accuracy at minibatch 104!\n",
      "148.1845222711563\n",
      "6524.0\n",
      "loss and accuracy at minibatch 105!\n",
      "149.51148962974548\n",
      "6584.0\n",
      "loss and accuracy at minibatch 106!\n",
      "150.8203489780426\n",
      "6652.0\n",
      "loss and accuracy at minibatch 107!\n",
      "152.0736345052719\n",
      "6722.0\n",
      "loss and accuracy at minibatch 108!\n",
      "153.28353238105774\n",
      "6793.0\n",
      "loss and accuracy at minibatch 109!\n",
      "154.79183840751648\n",
      "6843.0\n",
      "loss and accuracy at minibatch 110!\n",
      "155.979918718338\n",
      "6913.0\n",
      "loss and accuracy at minibatch 111!\n",
      "157.34683573246002\n",
      "6980.0\n",
      "loss and accuracy at minibatch 112!\n",
      "158.72133207321167\n",
      "7052.0\n",
      "loss and accuracy at minibatch 113!\n",
      "160.24687337875366\n",
      "7108.0\n",
      "loss and accuracy at minibatch 114!\n",
      "161.6581094264984\n",
      "7170.0\n",
      "loss and accuracy at minibatch 115!\n",
      "163.06829249858856\n",
      "7236.0\n",
      "loss and accuracy at minibatch 116!\n",
      "164.37609696388245\n",
      "7305.0\n",
      "loss and accuracy at minibatch 117!\n",
      "165.73145973682404\n",
      "7372.0\n",
      "loss and accuracy at minibatch 118!\n",
      "167.0945861339569\n",
      "7439.0\n",
      "loss and accuracy at minibatch 119!\n",
      "168.35406637191772\n",
      "7508.0\n",
      "loss and accuracy at minibatch 120!\n",
      "169.6067178249359\n",
      "7575.0\n",
      "loss and accuracy at minibatch 121!\n",
      "171.07413172721863\n",
      "7632.0\n",
      "loss and accuracy at minibatch 122!\n",
      "172.33533990383148\n",
      "7704.0\n",
      "loss and accuracy at minibatch 123!\n",
      "173.57406318187714\n",
      "7770.0\n",
      "loss and accuracy at minibatch 124!\n",
      "174.75880813598633\n",
      "7845.0\n",
      "loss and accuracy at minibatch 125!\n",
      "176.1348375082016\n",
      "7907.0\n",
      "loss and accuracy at minibatch 126!\n",
      "177.61606574058533\n",
      "7965.0\n",
      "loss and accuracy at minibatch 127!\n",
      "179.0957226753235\n",
      "8035.0\n",
      "loss and accuracy at minibatch 128!\n",
      "180.43939304351807\n",
      "8098.0\n",
      "loss and accuracy at minibatch 129!\n",
      "181.49577629566193\n",
      "8170.0\n",
      "loss and accuracy at minibatch 130!\n",
      "182.92302417755127\n",
      "8233.0\n",
      "loss and accuracy at minibatch 131!\n",
      "184.18555581569672\n",
      "8309.0\n",
      "loss and accuracy at minibatch 132!\n",
      "185.56110787391663\n",
      "8369.0\n",
      "loss and accuracy at minibatch 133!\n",
      "186.7955677509308\n",
      "8447.0\n",
      "loss and accuracy at minibatch 134!\n",
      "188.20006585121155\n",
      "8507.0\n",
      "loss and accuracy at minibatch 135!\n",
      "189.58396673202515\n",
      "8570.0\n",
      "loss and accuracy at minibatch 136!\n",
      "190.7638816833496\n",
      "8643.0\n",
      "loss and accuracy at minibatch 137!\n",
      "192.13183093070984\n",
      "8704.0\n",
      "loss and accuracy at minibatch 138!\n",
      "193.42601871490479\n",
      "8772.0\n",
      "loss and accuracy at minibatch 139!\n",
      "194.66928446292877\n",
      "8843.0\n",
      "loss and accuracy at minibatch 140!\n",
      "195.94835209846497\n",
      "8907.0\n",
      "loss and accuracy at minibatch 141!\n",
      "197.1835012435913\n",
      "8983.0\n",
      "loss and accuracy at minibatch 142!\n",
      "198.56772708892822\n",
      "9053.0\n",
      "loss and accuracy at minibatch 143!\n",
      "200.05177426338196\n",
      "9117.0\n",
      "loss and accuracy at minibatch 144!\n",
      "201.37719106674194\n",
      "9183.0\n",
      "loss and accuracy at minibatch 145!\n",
      "202.71490168571472\n",
      "9246.0\n",
      "loss and accuracy at minibatch 146!\n",
      "204.16969108581543\n",
      "9306.0\n",
      "loss and accuracy at minibatch 147!\n",
      "205.46432387828827\n",
      "9372.0\n",
      "loss and accuracy at minibatch 148!\n",
      "206.7604694366455\n",
      "9438.0\n",
      "loss and accuracy at minibatch 149!\n",
      "208.1111409664154\n",
      "9500.0\n",
      "loss and accuracy at minibatch 150!\n",
      "209.28366148471832\n",
      "9579.0\n",
      "loss and accuracy at minibatch 151!\n",
      "210.63657450675964\n",
      "9642.0\n",
      "loss and accuracy at minibatch 152!\n",
      "211.85394847393036\n",
      "9713.0\n",
      "loss and accuracy at minibatch 153!\n",
      "213.23278737068176\n",
      "9774.0\n",
      "loss and accuracy at minibatch 154!\n",
      "214.60575449466705\n",
      "9831.0\n",
      "loss and accuracy at minibatch 155!\n",
      "215.95744466781616\n",
      "9898.0\n",
      "loss and accuracy at minibatch 156!\n",
      "217.23916828632355\n",
      "9964.0\n",
      "loss and accuracy at minibatch 157!\n",
      "218.5613635778427\n",
      "10028.0\n",
      "loss and accuracy at minibatch 158!\n",
      "219.91861867904663\n",
      "10096.0\n",
      "loss and accuracy at minibatch 159!\n",
      "221.2769740819931\n",
      "10160.0\n",
      "loss and accuracy at minibatch 160!\n",
      "222.71657705307007\n",
      "10225.0\n",
      "loss and accuracy at minibatch 161!\n",
      "223.97792053222656\n",
      "10297.0\n",
      "loss and accuracy at minibatch 162!\n",
      "225.27554154396057\n",
      "10375.0\n",
      "loss and accuracy at minibatch 163!\n",
      "226.60049951076508\n",
      "10436.0\n",
      "loss and accuracy at minibatch 164!\n",
      "227.89273846149445\n",
      "10502.0\n",
      "loss and accuracy at minibatch 165!\n",
      "229.30884659290314\n",
      "10561.0\n",
      "loss and accuracy at minibatch 166!\n",
      "230.5947083234787\n",
      "10629.0\n",
      "loss and accuracy at minibatch 167!\n",
      "231.90907108783722\n",
      "10700.0\n",
      "loss and accuracy at minibatch 168!\n",
      "233.23520076274872\n",
      "10772.0\n",
      "loss and accuracy at minibatch 169!\n",
      "234.58937084674835\n",
      "10839.0\n",
      "loss and accuracy at minibatch 170!\n",
      "235.75383353233337\n",
      "10902.0\n",
      "loss and accuracy at minibatch 171!\n",
      "236.96292972564697\n",
      "10974.0\n",
      "loss and accuracy at minibatch 172!\n",
      "238.08411288261414\n",
      "11053.0\n",
      "loss and accuracy at minibatch 173!\n",
      "239.2803807258606\n",
      "11124.0\n",
      "loss and accuracy at minibatch 174!\n",
      "240.708691239357\n",
      "11184.0\n",
      "loss and accuracy at minibatch 175!\n",
      "241.7662912607193\n",
      "11263.0\n",
      "loss and accuracy at minibatch 176!\n",
      "243.07666170597076\n",
      "11332.0\n",
      "loss and accuracy at minibatch 177!\n",
      "244.49357426166534\n",
      "11401.0\n",
      "loss and accuracy at minibatch 178!\n",
      "245.72115755081177\n",
      "11478.0\n",
      "loss and accuracy at minibatch 179!\n",
      "246.9523959159851\n",
      "11550.0\n",
      "loss and accuracy at minibatch 180!\n",
      "248.26344990730286\n",
      "11613.0\n",
      "loss and accuracy at minibatch 181!\n",
      "249.4519922733307\n",
      "11680.0\n",
      "loss and accuracy at minibatch 182!\n",
      "250.74893879890442\n",
      "11751.0\n",
      "loss and accuracy at minibatch 183!\n",
      "251.96034729480743\n",
      "11825.0\n",
      "loss and accuracy at minibatch 184!\n",
      "253.21304380893707\n",
      "11896.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss and accuracy at minibatch 185!\n",
      "254.3712249994278\n",
      "11969.0\n",
      "loss and accuracy at minibatch 186!\n",
      "255.6444352865219\n",
      "12036.0\n",
      "loss and accuracy at minibatch 187!\n",
      "256.895902633667\n",
      "12107.0\n",
      "loss and accuracy at minibatch 188!\n",
      "258.2631764411926\n",
      "12170.0\n",
      "loss and accuracy at minibatch 189!\n",
      "259.51147079467773\n",
      "12244.0\n",
      "loss and accuracy at minibatch 190!\n",
      "260.70962953567505\n",
      "12314.0\n",
      "loss and accuracy at minibatch 191!\n",
      "261.89272463321686\n",
      "12392.0\n",
      "loss and accuracy at minibatch 192!\n",
      "263.18753921985626\n",
      "12464.0\n",
      "loss and accuracy at minibatch 193!\n",
      "264.4552649259567\n",
      "12536.0\n",
      "loss and accuracy at minibatch 194!\n",
      "265.7135854959488\n",
      "12599.0\n",
      "loss and accuracy at minibatch 195!\n",
      "266.8944387435913\n",
      "12674.0\n",
      "loss and accuracy at minibatch 196!\n",
      "268.33355939388275\n",
      "12735.0\n",
      "loss and accuracy at minibatch 197!\n",
      "269.59553480148315\n",
      "12807.0\n",
      "loss and accuracy at minibatch 198!\n",
      "270.7243592739105\n",
      "12891.0\n",
      "loss and accuracy at minibatch 199!\n",
      "271.9173879623413\n",
      "12968.0\n",
      "loss and accuracy at minibatch 200!\n",
      "273.1167846918106\n",
      "13046.0\n",
      "loss and accuracy at minibatch 201!\n",
      "274.2526191473007\n",
      "13118.0\n",
      "loss and accuracy at minibatch 202!\n",
      "275.4650448560715\n",
      "13191.0\n",
      "loss and accuracy at minibatch 203!\n",
      "276.75398576259613\n",
      "13259.0\n",
      "loss and accuracy at minibatch 204!\n",
      "278.07417595386505\n",
      "13328.0\n",
      "loss and accuracy at minibatch 205!\n",
      "279.5382124185562\n",
      "13386.0\n",
      "loss and accuracy at minibatch 206!\n",
      "280.98682951927185\n",
      "13444.0\n",
      "loss and accuracy at minibatch 207!\n",
      "282.2736202478409\n",
      "13516.0\n",
      "loss and accuracy at minibatch 208!\n",
      "283.381831407547\n",
      "13597.0\n",
      "loss and accuracy at minibatch 209!\n",
      "284.67613995075226\n",
      "13670.0\n",
      "loss and accuracy at minibatch 210!\n",
      "286.04749584198\n",
      "13733.0\n",
      "loss and accuracy at minibatch 211!\n",
      "287.312989115715\n",
      "13808.0\n",
      "loss and accuracy at minibatch 212!\n",
      "288.4744178056717\n",
      "13878.0\n",
      "loss and accuracy at minibatch 213!\n",
      "289.7500755786896\n",
      "13948.0\n",
      "loss and accuracy at minibatch 214!\n",
      "291.12406051158905\n",
      "14009.0\n",
      "loss and accuracy at minibatch 215!\n",
      "292.38746416568756\n",
      "14087.0\n",
      "loss and accuracy at minibatch 216!\n",
      "293.5468804836273\n",
      "14159.0\n",
      "loss and accuracy at minibatch 217!\n",
      "294.85212099552155\n",
      "14227.0\n",
      "loss and accuracy at minibatch 218!\n",
      "296.20012640953064\n",
      "14293.0\n",
      "loss and accuracy at minibatch 219!\n",
      "297.53593599796295\n",
      "14360.0\n",
      "loss and accuracy at minibatch 220!\n",
      "298.6581577062607\n",
      "14433.0\n",
      "loss and accuracy at minibatch 221!\n",
      "300.0036120414734\n",
      "14501.0\n",
      "loss and accuracy at minibatch 222!\n",
      "301.2588942050934\n",
      "14571.0\n",
      "loss and accuracy at minibatch 223!\n",
      "302.4841376543045\n",
      "14640.0\n",
      "loss and accuracy at minibatch 224!\n",
      "303.54737865924835\n",
      "14723.0\n",
      "loss and accuracy at minibatch 225!\n",
      "304.7854640483856\n",
      "14792.0\n",
      "loss and accuracy at minibatch 226!\n",
      "305.97057008743286\n",
      "14869.0\n",
      "loss and accuracy at minibatch 227!\n",
      "306.97148394584656\n",
      "14947.0\n",
      "loss and accuracy at minibatch 228!\n",
      "308.0060429573059\n",
      "15029.0\n",
      "loss and accuracy at minibatch 229!\n",
      "309.3654319047928\n",
      "15100.0\n",
      "loss and accuracy at minibatch 230!\n",
      "310.53484773635864\n",
      "15175.0\n",
      "loss and accuracy at minibatch 231!\n",
      "311.77484369277954\n",
      "15245.0\n",
      "loss and accuracy at minibatch 232!\n",
      "313.03521728515625\n",
      "15316.0\n",
      "loss and accuracy at minibatch 233!\n",
      "314.2335970401764\n",
      "15388.0\n",
      "loss and accuracy at minibatch 234!\n",
      "315.3792326450348\n",
      "15460.0\n",
      "loss and accuracy at minibatch 235!\n",
      "316.579687833786\n",
      "15530.0\n",
      "loss and accuracy at minibatch 236!\n",
      "317.89511728286743\n",
      "15606.0\n",
      "loss and accuracy at minibatch 237!\n",
      "319.08855867385864\n",
      "15674.0\n",
      "loss and accuracy at minibatch 238!\n",
      "320.408952832222\n",
      "15741.0\n",
      "loss and accuracy at minibatch 239!\n",
      "321.65714836120605\n",
      "15817.0\n",
      "loss and accuracy at minibatch 240!\n",
      "322.9059419631958\n",
      "15894.0\n",
      "loss and accuracy at minibatch 241!\n",
      "324.14946484565735\n",
      "15964.0\n",
      "loss and accuracy at minibatch 242!\n",
      "325.40124094486237\n",
      "16041.0\n",
      "loss and accuracy at minibatch 243!\n",
      "326.5217170715332\n",
      "16118.0\n",
      "loss and accuracy at minibatch 244!\n",
      "327.78600883483887\n",
      "16186.0\n",
      "loss and accuracy at minibatch 245!\n",
      "328.96435129642487\n",
      "16264.0\n",
      "loss and accuracy at minibatch 246!\n",
      "330.0837472677231\n",
      "16342.0\n",
      "loss and accuracy at minibatch 247!\n",
      "331.31820809841156\n",
      "16409.0\n",
      "loss and accuracy at minibatch 248!\n",
      "332.58458399772644\n",
      "16480.0\n",
      "loss and accuracy at minibatch 249!\n",
      "333.87864553928375\n",
      "16549.0\n",
      "loss and accuracy at minibatch 250!\n",
      "334.93041694164276\n",
      "16626.0\n",
      "loss and accuracy at minibatch 251!\n",
      "335.9431885480881\n",
      "16711.0\n",
      "loss and accuracy at minibatch 252!\n",
      "337.073508143425\n",
      "16779.0\n",
      "loss and accuracy at minibatch 253!\n",
      "338.4537888765335\n",
      "16844.0\n",
      "loss and accuracy at minibatch 254!\n",
      "339.7341752052307\n",
      "16912.0\n",
      "loss and accuracy at minibatch 255!\n",
      "340.64380383491516\n",
      "16995.0\n",
      "loss and accuracy at minibatch 256!\n",
      "341.8579262495041\n",
      "17073.0\n",
      "loss and accuracy at minibatch 257!\n",
      "342.8817398548126\n",
      "17155.0\n",
      "loss and accuracy at minibatch 258!\n",
      "344.2182239294052\n",
      "17222.0\n",
      "loss and accuracy at minibatch 259!\n",
      "345.1788822412491\n",
      "17304.0\n",
      "loss and accuracy at minibatch 260!\n",
      "346.5648933649063\n",
      "17375.0\n",
      "loss and accuracy at minibatch 261!\n",
      "347.8478591442108\n",
      "17446.0\n",
      "loss and accuracy at minibatch 262!\n",
      "349.0356386899948\n",
      "17513.0\n",
      "loss and accuracy at minibatch 263!\n",
      "350.4781427383423\n",
      "17578.0\n",
      "loss and accuracy at minibatch 264!\n",
      "351.5949339866638\n",
      "17656.0\n",
      "loss and accuracy at minibatch 265!\n",
      "352.85894107818604\n",
      "17722.0\n",
      "loss and accuracy at minibatch 266!\n",
      "354.07421612739563\n",
      "17788.0\n",
      "loss and accuracy at minibatch 267!\n",
      "355.0230012536049\n",
      "17874.0\n",
      "loss and accuracy at minibatch 268!\n",
      "356.37563413381577\n",
      "17933.0\n",
      "loss and accuracy at minibatch 269!\n",
      "357.5087931752205\n",
      "18013.0\n",
      "loss and accuracy at minibatch 270!\n",
      "358.48555916547775\n",
      "18101.0\n",
      "loss and accuracy at minibatch 271!\n",
      "359.67240875959396\n",
      "18176.0\n",
      "loss and accuracy at minibatch 272!\n",
      "360.9494978785515\n",
      "18246.0\n",
      "loss and accuracy at minibatch 273!\n",
      "362.14539831876755\n",
      "18319.0\n",
      "loss and accuracy at minibatch 274!\n",
      "363.12310618162155\n",
      "18405.0\n",
      "loss and accuracy at minibatch 275!\n",
      "364.17318147420883\n",
      "18486.0\n",
      "loss and accuracy at minibatch 276!\n",
      "365.39289313554764\n",
      "18558.0\n",
      "loss and accuracy at minibatch 277!\n",
      "366.46110969781876\n",
      "18629.0\n",
      "loss and accuracy at minibatch 278!\n",
      "367.57492738962173\n",
      "18701.0\n",
      "loss and accuracy at minibatch 279!\n",
      "368.5683644413948\n",
      "18788.0\n",
      "loss and accuracy at minibatch 280!\n",
      "369.8704243302345\n",
      "18856.0\n",
      "loss and accuracy at minibatch 281!\n",
      "371.04994839429855\n",
      "18928.0\n",
      "loss and accuracy at minibatch 282!\n",
      "372.50669568777084\n",
      "18989.0\n",
      "loss and accuracy at minibatch 283!\n",
      "373.35481202602386\n",
      "19083.0\n",
      "loss and accuracy at minibatch 284!\n",
      "374.4874188899994\n",
      "19168.0\n",
      "loss and accuracy at minibatch 285!\n",
      "375.69444274902344\n",
      "19244.0\n",
      "loss and accuracy at minibatch 286!\n",
      "376.9790313243866\n",
      "19313.0\n",
      "loss and accuracy at minibatch 287!\n",
      "378.1903487443924\n",
      "19389.0\n",
      "loss and accuracy at minibatch 288!\n",
      "379.4340261220932\n",
      "19459.0\n",
      "loss and accuracy at minibatch 289!\n",
      "380.69578444957733\n",
      "19537.0\n",
      "loss and accuracy at minibatch 290!\n",
      "381.92547273635864\n",
      "19607.0\n",
      "loss and accuracy at minibatch 291!\n",
      "383.15211606025696\n",
      "19681.0\n",
      "loss and accuracy at minibatch 292!\n",
      "384.3063460588455\n",
      "19756.0\n",
      "loss and accuracy at minibatch 293!\n",
      "385.4696190357208\n",
      "19826.0\n",
      "loss and accuracy at minibatch 294!\n",
      "386.7433284521103\n",
      "19893.0\n",
      "loss and accuracy at minibatch 295!\n",
      "387.94023537635803\n",
      "19969.0\n",
      "loss and accuracy at minibatch 296!\n",
      "389.1214189529419\n",
      "20037.0\n",
      "loss and accuracy at minibatch 297!\n",
      "390.4242641925812\n",
      "20104.0\n",
      "loss and accuracy at minibatch 298!\n",
      "391.64156198501587\n",
      "20173.0\n",
      "loss and accuracy at minibatch 299!\n",
      "392.7745794057846\n",
      "20250.0\n",
      "loss and accuracy at minibatch 300!\n",
      "393.9910817146301\n",
      "20322.0\n",
      "loss and accuracy at minibatch 301!\n",
      "395.35375440120697\n",
      "20387.0\n",
      "loss and accuracy at minibatch 302!\n",
      "396.5610307455063\n",
      "20463.0\n",
      "loss and accuracy at minibatch 303!\n",
      "397.72871351242065\n",
      "20540.0\n",
      "loss and accuracy at minibatch 304!\n",
      "398.9099667072296\n",
      "20610.0\n",
      "loss and accuracy at minibatch 305!\n",
      "400.0638859272003\n",
      "20686.0\n",
      "loss and accuracy at minibatch 306!\n",
      "401.3215363025665\n",
      "20755.0\n",
      "loss and accuracy at minibatch 307!\n",
      "402.53738510608673\n",
      "20824.0\n",
      "loss and accuracy at minibatch 308!\n",
      "403.84168803691864\n",
      "20894.0\n",
      "loss and accuracy at minibatch 309!\n",
      "404.9037346839905\n",
      "20977.0\n",
      "loss and accuracy at minibatch 310!\n",
      "405.94171822071075\n",
      "21052.0\n",
      "loss and accuracy at minibatch 311!\n",
      "407.18562495708466\n",
      "21124.0\n",
      "loss and accuracy at minibatch 312!\n",
      "408.2141889333725\n",
      "21206.0\n",
      "loss and accuracy at minibatch 313!\n",
      "409.61148715019226\n",
      "21266.0\n",
      "loss and accuracy at minibatch 314!\n",
      "410.6961486339569\n",
      "21346.0\n",
      "loss and accuracy at minibatch 315!\n",
      "412.10313308238983\n",
      "21405.0\n",
      "loss and accuracy at minibatch 316!\n",
      "413.3051997423172\n",
      "21478.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss and accuracy at minibatch 317!\n",
      "414.6287052631378\n",
      "21544.0\n",
      "loss and accuracy at minibatch 318!\n",
      "415.93772530555725\n",
      "21612.0\n",
      "loss and accuracy at minibatch 319!\n",
      "417.039888381958\n",
      "21688.0\n",
      "loss and accuracy at minibatch 320!\n",
      "418.3107134103775\n",
      "21759.0\n",
      "loss and accuracy at minibatch 321!\n",
      "419.37689876556396\n",
      "21839.0\n",
      "loss and accuracy at minibatch 322!\n",
      "420.4750579595566\n",
      "21918.0\n",
      "loss and accuracy at minibatch 323!\n",
      "421.4284802079201\n",
      "21998.0\n",
      "loss and accuracy at minibatch 324!\n",
      "422.5476250052452\n",
      "22083.0\n",
      "loss and accuracy at minibatch 325!\n",
      "423.87533539533615\n",
      "22150.0\n",
      "loss and accuracy at minibatch 326!\n",
      "425.01105231046677\n",
      "22219.0\n",
      "loss and accuracy at minibatch 327!\n",
      "426.16000813245773\n",
      "22291.0\n",
      "loss and accuracy at minibatch 328!\n",
      "427.10552990436554\n",
      "22376.0\n",
      "loss and accuracy at minibatch 329!\n",
      "428.33682239055634\n",
      "22450.0\n",
      "loss and accuracy at minibatch 330!\n",
      "429.5208646059036\n",
      "22531.0\n",
      "loss and accuracy at minibatch 331!\n",
      "430.5800074338913\n",
      "22609.0\n",
      "loss and accuracy at minibatch 332!\n",
      "431.6504135131836\n",
      "22684.0\n",
      "loss and accuracy at minibatch 333!\n",
      "432.72879707813263\n",
      "22765.0\n",
      "loss and accuracy at minibatch 334!\n",
      "433.91896319389343\n",
      "22831.0\n",
      "loss and accuracy at minibatch 335!\n",
      "435.1441128253937\n",
      "22896.0\n",
      "loss and accuracy at minibatch 336!\n",
      "436.4137017726898\n",
      "22969.0\n",
      "loss and accuracy at minibatch 337!\n",
      "437.33078891038895\n",
      "23061.0\n",
      "loss and accuracy at minibatch 338!\n",
      "438.3955325484276\n",
      "23144.0\n",
      "loss and accuracy at minibatch 339!\n",
      "439.4629574418068\n",
      "23224.0\n",
      "loss and accuracy at minibatch 340!\n",
      "440.54732090234756\n",
      "23304.0\n",
      "loss and accuracy at minibatch 341!\n",
      "441.981935441494\n",
      "23358.0\n",
      "loss and accuracy at minibatch 342!\n",
      "443.08949798345566\n",
      "23434.0\n",
      "loss and accuracy at minibatch 343!\n",
      "444.0828677415848\n",
      "23515.0\n",
      "loss and accuracy at minibatch 344!\n",
      "445.3230024576187\n",
      "23592.0\n",
      "loss and accuracy at minibatch 345!\n",
      "446.350759267807\n",
      "23670.0\n",
      "loss and accuracy at minibatch 346!\n",
      "447.43865942955017\n",
      "23745.0\n",
      "loss and accuracy at minibatch 347!\n",
      "448.6314296722412\n",
      "23818.0\n",
      "loss and accuracy at minibatch 348!\n",
      "449.6823878288269\n",
      "23894.0\n",
      "loss and accuracy at minibatch 349!\n",
      "450.7769972085953\n",
      "23973.0\n",
      "loss and accuracy at minibatch 350!\n",
      "451.8311754465103\n",
      "24047.0\n",
      "loss and accuracy at minibatch 351!\n",
      "452.9405610561371\n",
      "24085.0\n",
      "Epoch   2/200, train loss: 1.01e-02, accuracy: 53.52%\n",
      "Epoch   2/200, val loss: 2.82e-02, accuracy: 13.92%\n",
      "loss and accuracy at minibatch 0!\n",
      "0.9574593901634216\n",
      "84.0\n",
      "loss and accuracy at minibatch 1!\n",
      "2.049760401248932\n",
      "162.0\n",
      "loss and accuracy at minibatch 2!\n",
      "3.2530381083488464\n",
      "237.0\n",
      "loss and accuracy at minibatch 3!\n",
      "4.302394926548004\n",
      "316.0\n",
      "loss and accuracy at minibatch 4!\n",
      "5.3593849539756775\n",
      "398.0\n",
      "loss and accuracy at minibatch 5!\n",
      "6.405260741710663\n",
      "477.0\n",
      "loss and accuracy at minibatch 6!\n",
      "7.5303831696510315\n",
      "550.0\n",
      "loss and accuracy at minibatch 7!\n",
      "8.647007167339325\n",
      "627.0\n",
      "loss and accuracy at minibatch 8!\n",
      "9.740792095661163\n",
      "705.0\n",
      "loss and accuracy at minibatch 9!\n",
      "10.815201818943024\n",
      "784.0\n",
      "loss and accuracy at minibatch 10!\n",
      "12.008344948291779\n",
      "851.0\n",
      "loss and accuracy at minibatch 11!\n",
      "13.194242298603058\n",
      "925.0\n",
      "loss and accuracy at minibatch 12!\n",
      "14.260996043682098\n",
      "998.0\n",
      "loss and accuracy at minibatch 13!\n",
      "15.252371728420258\n",
      "1079.0\n",
      "loss and accuracy at minibatch 14!\n",
      "16.302049577236176\n",
      "1157.0\n",
      "loss and accuracy at minibatch 15!\n",
      "17.130249738693237\n",
      "1247.0\n",
      "loss and accuracy at minibatch 16!\n",
      "18.393741011619568\n",
      "1323.0\n",
      "loss and accuracy at minibatch 17!\n",
      "19.395897150039673\n",
      "1404.0\n",
      "loss and accuracy at minibatch 18!\n",
      "20.5724778175354\n",
      "1474.0\n",
      "loss and accuracy at minibatch 19!\n",
      "21.584681749343872\n",
      "1552.0\n",
      "loss and accuracy at minibatch 20!\n",
      "22.61944580078125\n",
      "1633.0\n",
      "loss and accuracy at minibatch 21!\n",
      "23.65453267097473\n",
      "1715.0\n",
      "loss and accuracy at minibatch 22!\n",
      "24.865962386131287\n",
      "1786.0\n",
      "loss and accuracy at minibatch 23!\n",
      "25.883532881736755\n",
      "1864.0\n",
      "loss and accuracy at minibatch 24!\n",
      "27.02343761920929\n",
      "1942.0\n",
      "loss and accuracy at minibatch 25!\n",
      "28.16370952129364\n",
      "2014.0\n",
      "loss and accuracy at minibatch 26!\n",
      "29.23628520965576\n",
      "2095.0\n",
      "loss and accuracy at minibatch 27!\n",
      "30.377640962600708\n",
      "2169.0\n",
      "loss and accuracy at minibatch 28!\n",
      "31.595020055770874\n",
      "2244.0\n",
      "loss and accuracy at minibatch 29!\n",
      "32.78749465942383\n",
      "2317.0\n",
      "loss and accuracy at minibatch 30!\n",
      "34.096802949905396\n",
      "2385.0\n",
      "loss and accuracy at minibatch 31!\n",
      "34.99570572376251\n",
      "2470.0\n",
      "loss and accuracy at minibatch 32!\n",
      "36.201040506362915\n",
      "2540.0\n",
      "loss and accuracy at minibatch 33!\n",
      "37.359705686569214\n",
      "2616.0\n",
      "loss and accuracy at minibatch 34!\n",
      "38.35348665714264\n",
      "2699.0\n",
      "loss and accuracy at minibatch 35!\n",
      "39.320510268211365\n",
      "2780.0\n",
      "loss and accuracy at minibatch 36!\n",
      "40.34493160247803\n",
      "2860.0\n",
      "loss and accuracy at minibatch 37!\n",
      "41.5810729265213\n",
      "2927.0\n",
      "loss and accuracy at minibatch 38!\n",
      "42.54381322860718\n",
      "3011.0\n",
      "loss and accuracy at minibatch 39!\n",
      "43.58298861980438\n",
      "3089.0\n",
      "loss and accuracy at minibatch 40!\n",
      "44.70652508735657\n",
      "3163.0\n",
      "loss and accuracy at minibatch 41!\n",
      "45.84643483161926\n",
      "3237.0\n",
      "loss and accuracy at minibatch 42!\n",
      "46.968079805374146\n",
      "3312.0\n",
      "loss and accuracy at minibatch 43!\n",
      "48.02197074890137\n",
      "3393.0\n",
      "loss and accuracy at minibatch 44!\n",
      "48.99756932258606\n",
      "3470.0\n",
      "loss and accuracy at minibatch 45!\n",
      "50.19790434837341\n",
      "3547.0\n",
      "loss and accuracy at minibatch 46!\n",
      "51.21854746341705\n",
      "3625.0\n",
      "loss and accuracy at minibatch 47!\n",
      "52.459105134010315\n",
      "3700.0\n",
      "loss and accuracy at minibatch 48!\n",
      "53.39429444074631\n",
      "3784.0\n",
      "loss and accuracy at minibatch 49!\n",
      "54.58150714635849\n",
      "3857.0\n",
      "loss and accuracy at minibatch 50!\n",
      "55.59307485818863\n",
      "3944.0\n",
      "loss and accuracy at minibatch 51!\n",
      "56.66830211877823\n",
      "4025.0\n",
      "loss and accuracy at minibatch 52!\n",
      "57.831269919872284\n",
      "4101.0\n",
      "loss and accuracy at minibatch 53!\n",
      "59.02257400751114\n",
      "4172.0\n",
      "loss and accuracy at minibatch 54!\n",
      "60.0369468331337\n",
      "4257.0\n",
      "loss and accuracy at minibatch 55!\n",
      "61.05404847860336\n",
      "4335.0\n",
      "loss and accuracy at minibatch 56!\n",
      "62.21654576063156\n",
      "4412.0\n",
      "loss and accuracy at minibatch 57!\n",
      "63.33309942483902\n",
      "4487.0\n",
      "loss and accuracy at minibatch 58!\n",
      "64.28550261259079\n",
      "4571.0\n",
      "loss and accuracy at minibatch 59!\n",
      "65.3413913846016\n",
      "4647.0\n",
      "loss and accuracy at minibatch 60!\n",
      "66.48099488019943\n",
      "4725.0\n",
      "loss and accuracy at minibatch 61!\n",
      "67.70763140916824\n",
      "4802.0\n",
      "loss and accuracy at minibatch 62!\n",
      "68.81991893053055\n",
      "4878.0\n",
      "loss and accuracy at minibatch 63!\n",
      "69.98139435052872\n",
      "4946.0\n",
      "loss and accuracy at minibatch 64!\n",
      "70.9009780883789\n",
      "5033.0\n",
      "loss and accuracy at minibatch 65!\n",
      "71.99648761749268\n",
      "5113.0\n",
      "loss and accuracy at minibatch 66!\n",
      "73.054248213768\n",
      "5197.0\n",
      "loss and accuracy at minibatch 67!\n",
      "74.04441159963608\n",
      "5280.0\n",
      "loss and accuracy at minibatch 68!\n",
      "75.11612623929977\n",
      "5357.0\n",
      "loss and accuracy at minibatch 69!\n",
      "76.26329165697098\n",
      "5431.0\n",
      "loss and accuracy at minibatch 70!\n",
      "77.45422905683517\n",
      "5506.0\n",
      "loss and accuracy at minibatch 71!\n",
      "78.60813301801682\n",
      "5586.0\n",
      "loss and accuracy at minibatch 72!\n",
      "79.61188191175461\n",
      "5666.0\n",
      "loss and accuracy at minibatch 73!\n",
      "80.63229495286942\n",
      "5745.0\n",
      "loss and accuracy at minibatch 74!\n",
      "81.5611863732338\n",
      "5829.0\n",
      "loss and accuracy at minibatch 75!\n",
      "82.59562891721725\n",
      "5913.0\n",
      "loss and accuracy at minibatch 76!\n",
      "83.8205036520958\n",
      "5988.0\n",
      "loss and accuracy at minibatch 77!\n",
      "85.03824192285538\n",
      "6062.0\n",
      "loss and accuracy at minibatch 78!\n",
      "85.9858803153038\n",
      "6148.0\n",
      "loss and accuracy at minibatch 79!\n",
      "87.09657782316208\n",
      "6227.0\n",
      "loss and accuracy at minibatch 80!\n",
      "88.08591455221176\n",
      "6313.0\n",
      "loss and accuracy at minibatch 81!\n",
      "89.06823951005936\n",
      "6399.0\n",
      "loss and accuracy at minibatch 82!\n",
      "90.08704107999802\n",
      "6479.0\n",
      "loss and accuracy at minibatch 83!\n",
      "90.91528183221817\n",
      "6568.0\n",
      "loss and accuracy at minibatch 84!\n",
      "91.89086538553238\n",
      "6654.0\n",
      "loss and accuracy at minibatch 85!\n",
      "93.0867195725441\n",
      "6730.0\n",
      "loss and accuracy at minibatch 86!\n",
      "94.2346003651619\n",
      "6808.0\n",
      "loss and accuracy at minibatch 87!\n",
      "95.19986009597778\n",
      "6888.0\n",
      "loss and accuracy at minibatch 88!\n",
      "96.22353541851044\n",
      "6971.0\n",
      "loss and accuracy at minibatch 89!\n",
      "97.34207117557526\n",
      "7051.0\n",
      "loss and accuracy at minibatch 90!\n",
      "98.48343789577484\n",
      "7124.0\n",
      "loss and accuracy at minibatch 91!\n",
      "99.53039336204529\n",
      "7204.0\n",
      "loss and accuracy at minibatch 92!\n",
      "100.5238641500473\n",
      "7288.0\n",
      "loss and accuracy at minibatch 93!\n",
      "101.45213109254837\n",
      "7378.0\n",
      "loss and accuracy at minibatch 94!\n",
      "102.46614223718643\n",
      "7461.0\n",
      "loss and accuracy at minibatch 95!\n",
      "103.59623831510544\n",
      "7538.0\n",
      "loss and accuracy at minibatch 96!\n",
      "104.69828563928604\n",
      "7620.0\n",
      "loss and accuracy at minibatch 97!\n",
      "105.6406643986702\n",
      "7706.0\n",
      "loss and accuracy at minibatch 98!\n",
      "106.62045031785965\n",
      "7796.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss and accuracy at minibatch 99!\n",
      "107.45623028278351\n",
      "7884.0\n",
      "loss and accuracy at minibatch 100!\n",
      "108.4834463596344\n",
      "7963.0\n",
      "loss and accuracy at minibatch 101!\n",
      "109.37286394834518\n",
      "8059.0\n",
      "loss and accuracy at minibatch 102!\n",
      "110.54686731100082\n",
      "8134.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-d8472a5c7a54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-78-55fd625fca9f>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#print(\"going to run epoch\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-11a4241c18b8>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(model, optimizer, dataloader, train)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m#print(\"current epoch forward done\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m#print(\"current epoch backward done\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_install\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_install\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# Fit model\n",
    "print(\"start\")\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we run it on the test set to obtain our final accuracy.\n",
    "Note that we can only look at the test set once, everything else would lead to overfitting. So you _must_ ignore the test set while developing your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.8e-03, accuracy: 92.07%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "print(f\"Test loss: {test_loss:.1e}, accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost what was reported in the paper (92.49%) and we didn't even train on the full training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional task: Squeeze out all the juice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you do even better? Have a look at [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/) and some state-of-the-art architectures such as [EfficientNet architecture](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html). Play around with the possibilities PyTorch offers you and see how close you can get to the [state of the art on CIFAR-10](https://paperswithcode.com/sota/image-classification-on-cifar-10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
